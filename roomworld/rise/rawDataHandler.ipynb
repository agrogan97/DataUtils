{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle Raw Data\n",
    "###\n",
    "\n",
    "- *Expects*: PICKLE\n",
    "- *Returns*: CSV\n",
    "\n",
    "This file takes in the raw data, which has been downloaded from the server, and passed into the digital twin. It can then be downloaded from the download link. This is done for 2 reasons:\n",
    "- To reduce processing memory requires on the server\n",
    "- To use SQL to filter by date, and thus reduce memory load on analysis machines\n",
    "\n",
    "Running the cells in this notebook will do the following:\n",
    "- Clean the data and structure it into a per-participant format\n",
    "- De-duplicate the data, according to pre-specified criteria [TODO]\n",
    "- Parse the data into round-level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARAMETERS\n",
    "# ------------\n",
    "\n",
    "# Pickle path\n",
    "picklePath = \"1706270670.pickle\"\n",
    "# The path to where the csv should be saved\n",
    "csvPath = \"rw_24_all.csv\"\n",
    "# Path to the duplicate file\n",
    "duplicatesPath = \"duplicates.csv\"\n",
    "\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open as pickle\n",
    "with open(picklePath, 'rb') as handle:\n",
    "    qs = pickle.load(handle)\n",
    "\n",
    "jsonData = qs.decode('utf8').replace(\"'\", '\"')\n",
    "data = json.loads(jsonData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(rawData):\n",
    "    cleanData = {}\n",
    "    for user in rawData:\n",
    "        # pk = user['id']\n",
    "        # user = userAll['fields']\n",
    "        tmp = {\n",
    "            'pk' : user['id'],\n",
    "            'userId' : urllib.parse.quote(user['userId']),\n",
    "            'rawData' : json.loads(user['rawData']),\n",
    "            'sdata' : None,\n",
    "            # 'edata' : user['edata'],\n",
    "            # 'parameters' : user['parameters'],\n",
    "            'totalAttempts' : None,\n",
    "            'completed' : user['completeAttempt'],\n",
    "            # 'lastCompletedRound' : None,\n",
    "            # 'lastTrialGame' : None,\n",
    "            # 'finalRooms' : [],\n",
    "            # 'urlParameters' : user['urlParameters'],\n",
    "            'timestamps' : [],\n",
    "            'timeCreated' : user['timeCreated'],\n",
    "            'lastModified' : user['lastModified']\n",
    "        }\n",
    "        # Check how many attempts the user has had\n",
    "        tmp['totalAttempts'] = len(tmp['rawData'])\n",
    "        # Store timestamp/s\n",
    "        tmp['timestamps'] = list(tmp['rawData'].keys())\n",
    "        # If it's one, check if it's complete\n",
    "        if tmp['totalAttempts'] == 1:\n",
    "            # Get the attempt timestamp\n",
    "            timestamp = list(tmp['rawData'].keys())[0]\n",
    "        elif tmp['totalAttempts'] > 1:\n",
    "            # For multiple attempts, find the occurence with the highest number of completed trial_layouts\n",
    "            indAttempts = [] # individual attempts\n",
    "            for i in range(tmp['totalAttempts']):\n",
    "                # Get the sdata for this timestamp, and get the length of the expt_index array\n",
    "                try:\n",
    "                    attNum = len(json.loads(tmp['rawData'][tmp['timestamps'][i]]['sdata'])['expt_index'])\n",
    "                    indAttempts.append(attNum)\n",
    "                except:\n",
    "                    indAttempts.append(0)\n",
    "            # Get the index of the value with the greatest magnitude\n",
    "            timestamp = tmp['timestamps'][np.argmax(indAttempts)]\n",
    "            \n",
    "        # Use the timestamp to add sdata to tmp\n",
    "        try:\n",
    "            tmp['sdata'] = json.loads(tmp['rawData'][timestamp]['sdata'])\n",
    "        except:\n",
    "            if tmp['rawData'][timestamp]['sdata'] == None or len(tmp['rawData'][timestamp]['sdata']) == 0:\n",
    "                tmp['sdata'] = None\n",
    "        if tmp['sdata'] != None:\n",
    "            # Check if complete by:\n",
    "            #    - trial_layout == 92 or\n",
    "            #    - trial_game == 80\n",
    "            # if len(Counter(tmp['sdata']['trial_layout']).keys()) >= 92:\n",
    "            if max(np.array(tmp['sdata']['trial_game'], dtype=np.float64)) >= 80:\n",
    "                tmp['completed'] = True\n",
    "                tmp['lastCompletedRound'] = len(tmp['sdata']['trial_game'])\n",
    "            else:\n",
    "                tmp['completed'] = False\n",
    "                tmp['lastCompletedRound'] = len(Counter(tmp['sdata']['trial_layout']).keys())\n",
    "            # Store how many trial_games they've seen\n",
    "            tmp['lastTrialGame'] = int(tmp['sdata']['trial_game'][-1])\n",
    "        else:\n",
    "            tmp['completed'] = False\n",
    "            tmp['lastCompletedRound'] = 0\n",
    "            \n",
    "        cleanData[tmp['pk']] = tmp\n",
    "    print(\"Dataset ready.\")\n",
    "    return cleanData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRounds(cleanData):\n",
    "    rounds = {}\n",
    "    count = 0\n",
    "    for user in cleanData:\n",
    "        u = cleanData[user]\n",
    "        us = u['sdata']\n",
    "        if us is not None:\n",
    "            for i in range(len(u['sdata']['expt_index'])):\n",
    "                rounds[str(count)] = {\n",
    "                    \"pk\" : u['pk'],\n",
    "                    \"id\" : u['userId'],\n",
    "                    \"iv\" : None if (u['urlParameters'] is None or u['urlParameters']['iv'] == None) else urllib.parse.quote(u['urlParameters']['iv']),\n",
    "                    \"tag\" : None if (u['urlParameters'] is None or u['urlParameters']['tag'] == None) else urllib.parse.quote(u['urlParameters']['tag']),\n",
    "                    \"expt_index\": us['expt_index'][i],\n",
    "                    \"expt_trial\": us['expt_trial'][i],\n",
    "                    \"trial_layout\" : us['trial_layout'][i],\n",
    "                    \"trial_level\" : us['trial_level'][i],\n",
    "                    \"trial_solved\" : us['trial_solved'][i],\n",
    "                    \"trial_attempts\" : us['trial_attempts'][i],\n",
    "                    \"trial_game\" : us['trial_game'][i],\n",
    "                    \"trial_transfer\" : us['trial_transfer'][i],\n",
    "                    \"trial_test\" : us['trial_test'][i],\n",
    "                    \"round_start_time\" : None,\n",
    "                    \"round_end_time\" : None,\n",
    "                    \"last_room\" : None,\n",
    "                    \"roundAttempted\" : None,\n",
    "                    \"gameComplete\" : u[\"completed\"],\n",
    "                }\n",
    "\n",
    "                # Insert start time, end time, and roundAttempted\n",
    "                if len(us[\"resp\"][str(i)][\"timestamp\"]) != 0:\n",
    "                    # If an attempt has been made\n",
    "                    rounds[str(count)][\"round_start_time\"] = us[\"resp\"][str(i)][\"timestamp\"][0] - us[\"resp\"][str(i)][\"reactiontime\"][0]\n",
    "                    rounds[str(count)][\"round_end_time\"] = us[\"resp\"][str(i)][\"timestamp\"][-1]\n",
    "                    rounds[str(count)][\"roundAttempted\"] = True\n",
    "                else:\n",
    "                    rounds[str(count)][\"roundAttempted\"] = False\n",
    "                    \n",
    "                # Compute final room position as [x, y]. Scale is 0->10 and includes 2 walls\n",
    "                if len(us[\"resp\"][str(i)][\"xloc\"]) != 0 and len(us[\"resp\"][str(i)][\"yloc\"]) != 0:\n",
    "                    xloc = us[\"resp\"][str(i)][\"xloc\"][-1]\n",
    "                    yloc = us[\"resp\"][str(i)][\"yloc\"][-1]\n",
    "                    rounds[str(count)][\"last_room\"] = []\n",
    "                    if xloc < 3:\n",
    "                        rounds[str(count)][\"last_room\"].append(0)\n",
    "                    elif 3 < xloc < 7:\n",
    "                        rounds[str(count)][\"last_room\"].append(1)\n",
    "                    elif 7 < xloc < 11:\n",
    "                        rounds[str(count)][\"last_room\"].append(2)\n",
    "                    if yloc < 3:\n",
    "                        rounds[str(count)][\"last_room\"].append(0)\n",
    "                    elif 3 < yloc < 7:\n",
    "                        rounds[str(count)][\"last_room\"].append(1)\n",
    "                    elif 7 < yloc < 11:\n",
    "                        rounds[str(count)][\"last_room\"].append(2)\n",
    "                    rounds[str(count)][\"last_room\"] = str(rounds[str(count)][\"last_room\"]).replace(\",\", \"-\")\n",
    "\n",
    "                # Increase round count\n",
    "                count += 1\n",
    "\n",
    "    # Store as dataframe\n",
    "    rounds = pd.DataFrame.from_dict(rounds, orient=\"index\")\n",
    "\n",
    "    return rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready.\n"
     ]
    }
   ],
   "source": [
    "cleanData = clean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pk', 'userId', 'rawData', 'sdata', 'edata', 'parameters', 'totalAttempts', 'completed', 'lastCompletedRound', 'lastTrialGame', 'finalRooms', 'urlParameters', 'timestamps', 'timeCreated', 'lastModified'])\n"
     ]
    }
   ],
   "source": [
    "for i in cleanData:\n",
    "    print(cleanData[i].keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData_df = pd.DataFrame.from_dict(cleanData, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData_df.to_csv(\"../data/rise_year4_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDuplicates(cleanData, format):\n",
    "    \"\"\"\n",
    "    The most recent version of the deduplication processor\n",
    "    \"\"\"\n",
    "    duplicates = pd.read_csv(duplicatesPath)\n",
    "    format = format.lower()\n",
    "    assert format in [\"None\", None, \"research\", \"rise\", \"all\"], f\"Unrecognised format for deduplication: {format} - allowed types are None, RISE, research, all. See docs for more information.\"\n",
    "    # convert the cleanData from JSON to dataframe\n",
    "    clean_df = pd.DataFrame.from_dict(cleanData, orient=\"index\")\n",
    "    # Make a list of riseId : [userIds] that contains single-players, and duplicate players\n",
    "    counts = Counter(duplicates[\"USER_ID\"])\n",
    "    # map riseId to [userIds]\n",
    "    ids = {id : list(duplicates.loc[duplicates[\"USER_ID\"] == id][\"ID\"]) for id in counts}\n",
    "    # Get only duplicate users\n",
    "    duplicateHashes = {i : ids[i] for i in ids if len(ids[i]) >= 2}\n",
    "    print(f\"# Unique players: {len(ids)} -- # of those who repeated: {len(duplicateHashes)}\")\n",
    "\n",
    "    riseAttempts = [] # the first complete if available - if not, the first incomplete\n",
    "    researchAttempts = []\n",
    "    allAttempts = [] # for debugging, not important\n",
    "    # take the first complete attempt - if none found, take just the first incomplete attempt\n",
    "    # for each attempt made by one individual, check if they completed\n",
    "    mapping = []\n",
    "    i = 0\n",
    "    for attempt in ids:\n",
    "        # Get a df for just this participant\n",
    "        repeats = clean_df.loc[clean_df[\"userId\"].isin(ids[attempt])]\n",
    "        completes = repeats.loc[repeats[\"completed\"] == True]\n",
    "        if (len(repeats) == 0):\n",
    "            continue\n",
    "        if len(completes) == 0:\n",
    "            # no completed attempts, take just the first attempt\n",
    "            firstTimestamp = min(list(repeats[\"timeCreated\"]))\n",
    "            # Returns the hashed ID associated with the attempt\n",
    "            riseAttempts.append(list(repeats.loc[repeats[\"timeCreated\"] == firstTimestamp][\"userId\"])[0])\n",
    "            # Store a lookup of this hashed ID and the unique attempt number it was\n",
    "            mapping.append({\"userId\" : riseAttempts[-1], \"uniqueAttemptNumber\" : 0})\n",
    "        elif len(completes) == 1:\n",
    "            # if 1 complete attempt - NB: this could be in any position, not necessarily first attempted\n",
    "            riseAttempts.append(list(completes[\"userId\"])[0])\n",
    "            timestamp = list(completes[\"timeCreated\"])[0]\n",
    "            # This complete may not be the first attempt, so let's see which attempt number it is\n",
    "            # See which attempt number this is:\n",
    "            #   - Sort the timestamps in order, and get the index of our timestamp in that list\n",
    "            uniqueAttemptNumber = list(repeats[\"timeCreated\"].sort_values()).index(timestamp)\n",
    "            # Store a lookup of this hashed ID and the unique attempt number it was\n",
    "            mapping.append({\"userId\" : riseAttempts[-1], \"uniqueAttemptNumber\" : uniqueAttemptNumber})\n",
    "        else:\n",
    "            # if multiple complete attempts found - take the first for rise\n",
    "            firstTimestamp = min(list(completes[\"timeCreated\"]))\n",
    "            riseAttempts.append(list(completes.loc[completes[\"timeCreated\"] == firstTimestamp][\"userId\"])[0])\n",
    "            # Store a lookup of this hashed ID and the unique attempt number it was\n",
    "            mapping.append({\"userId\" : riseAttempts[-1], \"uniqueAttemptNumber\" : 0})\n",
    "\n",
    "        # if more than one attempt, save which attempt number in total this is\n",
    "        if len(repeats) > 1:\n",
    "            # order the creation timestamps\n",
    "            repeats[\"timeCreated\"].sort_values()\n",
    "            pass\n",
    "\n",
    "        # For research purposes, we want to take only the very first attempt\n",
    "        firstTimestamp = min(list(repeats[\"timeCreated\"]))\n",
    "        tmp = repeats.loc[repeats[\"timeCreated\"] == firstTimestamp][\"userId\"]\n",
    "\n",
    "        researchAttempts.append(list(tmp)[0])\n",
    "        allAttempts.append(ids[attempt])\n",
    "\n",
    "        # i += 1\n",
    "        # if i == 10:\n",
    "        #     break\n",
    "\n",
    "    if format == \"rise\":\n",
    "        # merge the id : unique attempt number mapping\n",
    "        mapping_df = pd.DataFrame.from_dict(mapping)\n",
    "        # keep only the rise ones in cleanData\n",
    "        return clean_df.loc[clean_df[\"userId\"].isin(list(riseAttempts))].merge(mapping_df)\n",
    "            \n",
    "    elif format == \"research\":\n",
    "        # keep only the research ones in cleanData\n",
    "        return clean_df.loc[clean_df[\"userId\"].isin(researchAttempts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unique players: 10126 -- # of those who repeated: 1763\n"
     ]
    }
   ],
   "source": [
    "dedupped_df = processDuplicates(cleanData, \"research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupped_df.to_csv(\"../data/rise_year4_clean_researchDeduplicated.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({True: 6568, False: 3556})"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(list(dedupped_df[\"completed\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDataDedupped = pd.DataFrame.to_dict(dedupped_df, orient=\"index\")\n",
    "rounds = getRounds(cleanDataDedupped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds.to_csv(\"../data/rise_year4_riseDeduplicated_rounds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready.\n"
     ]
    }
   ],
   "source": [
    "# Clean the raw data\n",
    "cleanData = clean(data)\n",
    "# Load in duplicate crosswalk\n",
    "duplicates = pd.read_csv(\"y2/duplicates.csv\")\n",
    "# Filter duplicates\n",
    "filteredCleanData = processDuplicates(cleanData, duplicates)\n",
    "# Convert filtered data from DF to dict\n",
    "cleanData = pd.DataFrame.to_dict(filteredCleanData, orient=\"index\")\n",
    "# Get round-level data\n",
    "rounds = getRounds(cleanData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds.to_csv(csvPath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({False: 6196, True: 6639})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(filteredCleanData[\"completed\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2af3e74c77804eabf4ed3fa3c408c8f0696463e74f76c3d29815837338750af1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
